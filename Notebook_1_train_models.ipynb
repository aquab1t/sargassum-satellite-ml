{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Training Sargassum Detection Models\n",
    "\n",
    "This notebook provides a step-by-step guide to training the Sargassum detection models. It mirrors `1_train_models.py` (Keras MLP + 1D CNN) and `1_train_models_ML.py` (XGBoost GPU).\n",
    "\n",
    "**Models Trained:**\n",
    "- **Keras MLP** — Mixed Precision float16, saved as `.keras` and quantized to TFLite INT8 / FLOAT16\n",
    "- **1D CNN** — Mixed Precision float16, saved as `.keras` and quantized to TFLite INT8 / FLOAT16\n",
    "- **XGBoost GPU** — GPU-accelerated, hyperparameter-tuned via `GridSearchCV`\n",
    "\n",
    "**Workflow:**\n",
    "1. Environment Setup\n",
    "2. Data Loading & Exploration\n",
    "3. Preprocessing\n",
    "4. Keras MLP Training\n",
    "5. 1D CNN Training\n",
    "6. TFLite Quantization (INT8 + FLOAT16)\n",
    "7. XGBoost GPU Training\n",
    "8. Evaluation & Performance Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies\n",
    "\n",
    "Install all required packages with:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Key Library Versions:**\n",
    "- `numpy==1.26.4`\n",
    "- `pandas==2.3.0`\n",
    "- `scikit-learn==1.5.2`\n",
    "- `xgboost==2.1.4`\n",
    "- `tensorflow==2.17.0`\n",
    "- `matplotlib==3.9.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Matplotlib Backend (before any pyplot import) ---\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# --- Standard Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Scikit-learn (utilities only) ---\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    ")\n",
    "\n",
    "# --- XGBoost ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print('XGBoost loaded successfully.')\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print('Warning: XGBoost not found. XGBoost training will be skipped.')\n",
    "\n",
    "# --- Reproducibility Seed ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- TensorFlow / Keras ---\n",
    "TF_AVAILABLE = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.metrics import Precision as KerasPrecision, Recall as KerasRecall\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    try:\n",
    "        AdamOptimizer = tf.keras.optimizers.Adam\n",
    "    except AttributeError:\n",
    "        AdamOptimizer = tf.keras.optimizers.legacy.Adam\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    # Configure GPU memory growth\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Set memory growth for {len(gpus)} GPU(s).')\n",
    "        except RuntimeError as e:\n",
    "            print(f'Error setting GPU memory growth: {e}')\n",
    "\n",
    "    # Enable mixed precision for faster training on compatible GPUs\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(f'Mixed precision policy: {policy.name}')\n",
    "    TF_AVAILABLE = True\n",
    "    print('TensorFlow/Keras loaded successfully.')\n",
    "except ImportError:\n",
    "    print('Warning: TensorFlow not found. Keras models will be skipped.')\n",
    "\n",
    "# --- Global Configuration ---\n",
    "CSV_PATH          = 'sargassum_data.csv'\n",
    "OUTPUT_DIR_MODELS = 'output/models_classification'\n",
    "OUTPUT_DIR_PLOTS  = 'output/plots_classification'\n",
    "OUTPUT_DIR_TABLES = 'output/tables_classification'\n",
    "POSITIVE_CLASS_LABEL = 'sargassum'\n",
    "CLASS_COLUMN      = 'class'\n",
    "TEST_SIZE         = 0.3\n",
    "\n",
    "# Keras training hyperparameters (matching 1_train_models.py)\n",
    "MLP_EPOCHS             = 100\n",
    "CNN_EPOCHS             = 100\n",
    "BATCH_SIZE             = 64\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "VALIDATION_SPLIT       = 0.2\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_MODELS, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_PLOTS,  exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_TABLES, exist_ok=True)\n",
    "print('Directories created. Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "We load the spectral library `sargassum_data.csv`, which contains ~196 K labelled pixel samples with 5 reflectance bands and a binary `class` column (`sargassum` / `no_sargassum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nClass distribution:')\n",
    "print(df[CLASS_COLUMN].value_counts())\n",
    "print(f'\\nClass balance ratio: {df[CLASS_COLUMN].value_counts().iloc[0] / df[CLASS_COLUMN].value_counts().iloc[1]:.1f}:1')\n",
    "\n",
    "print('\\nFirst 5 rows:')\n",
    "display(df.head())\n",
    "\n",
    "print('\\nFeature statistics:')\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing: Splitting and Scaling\n",
    "\n",
    "1. **Label Encoding** — convert string labels to integers (0 / 1).\n",
    "2. **Train-Test Split** — 70 / 30 stratified split (`SEED=42`).\n",
    "3. **Feature Scaling** — `StandardScaler` fit **only** on training data to prevent leakage.\n",
    "\n",
    "Both the scaler and label encoder are saved to `output/models_classification/` for use during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separate features and target\n",
    "feature_cols = [c for c in df.columns if c != CLASS_COLUMN]\n",
    "X = df[feature_cols].values.astype(np.float64)\n",
    "y_labels_str = df[CLASS_COLUMN].values\n",
    "\n",
    "# 2. Label encoding\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_labels_str)\n",
    "sargassum_encoded_value = le.transform([POSITIVE_CLASS_LABEL])[0]\n",
    "print(f'Class mapping: {list(zip(le.classes_, le.transform(le.classes_)))}')\n",
    "print(f'Positive class \"{POSITIVE_CLASS_LABEL}\" encoded as: {sargassum_encoded_value}')\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y\n",
    ")\n",
    "print(f'\\nTraining set: {X_train.shape[0]:,} samples')\n",
    "print(f'Test set:     {X_test.shape[0]:,} samples')\n",
    "\n",
    "# 4. Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler and label encoder\n",
    "joblib.dump(le,     os.path.join(OUTPUT_DIR_MODELS, 'label_encoder_classification.joblib'))\n",
    "joblib.dump(scaler, os.path.join(OUTPUT_DIR_MODELS, 'scaler_classification.joblib'))\n",
    "print('\\nLabelEncoder and Scaler saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MLP Training (Mixed Precision)\n",
    "\n",
    "A compact 3-layer MLP (100 → 50 → 1) trained with `float16` mixed precision and early stopping.\n",
    "The trained model is saved as `.keras` and will be quantized to TFLite in Section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model builder functions ───────────────────────────────────────────────────\n",
    "def build_mlp_classifier(input_shape):\n",
    "    \"\"\"Keras MLP for binary classification.\"\"\"\n",
    "    if not TF_AVAILABLE:\n",
    "        return None\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=AdamOptimizer(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', KerasPrecision(name='precision'), KerasRecall(name='recall')],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_cnn_classifier(input_shape):\n",
    "    \"\"\"1D CNN for binary classification.\"\"\"\n",
    "    if not TF_AVAILABLE:\n",
    "        return None\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2), Dropout(0.3),\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2), Dropout(0.4),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'), Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=AdamOptimizer(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', KerasPrecision(name='precision'), KerasRecall(name='recall')],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_cm(y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"Plot and save a confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=class_names).plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "    ax.set_title(f'Confusion Matrix - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR_PLOTS, f'cm_{model_name.lower().replace(\" \",\"_\")}.png'), dpi=150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ── Train Keras MLP ───────────────────────────────────────────────────────────\n",
    "results = {}\n",
    "mlp_model = None\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    print('--- Training Keras MLP (Mixed Precision) ---')\n",
    "    mlp_start = time.time()\n",
    "    try:\n",
    "        mlp_model = build_mlp_classifier((X_train_scaled.shape[1],))\n",
    "        mlp_model.summary()\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss', patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True, verbose=1,\n",
    "        )\n",
    "        mlp_model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=MLP_EPOCHS, batch_size=BATCH_SIZE,\n",
    "            validation_split=VALIDATION_SPLIT,\n",
    "            callbacks=[early_stop], verbose=2,\n",
    "        )\n",
    "        mlp_duration = time.time() - mlp_start\n",
    "        y_pred_mlp = (mlp_model.predict(X_test_scaled).flatten() > 0.5).astype(int)\n",
    "        results['Keras_MLP_Mixed'] = {\n",
    "            'Accuracy':       accuracy_score(y_test, y_pred_mlp),\n",
    "            'Precision (w)':  precision_score(y_test, y_pred_mlp, average='weighted', zero_division=0),\n",
    "            'Recall (w)':     recall_score(y_test, y_pred_mlp, average='weighted', zero_division=0),\n",
    "            'F1-Score (w)':   f1_score(y_test, y_pred_mlp, average='weighted', zero_division=0),\n",
    "            'Train Time (s)': mlp_duration,\n",
    "        }\n",
    "        print(classification_report(y_test, y_pred_mlp, target_names=le.classes_, digits=4))\n",
    "        plot_cm(y_test, y_pred_mlp, le.classes_, 'Keras_MLP_Mixed')\n",
    "        mlp_path = os.path.join(OUTPUT_DIR_MODELS, 'mlp_classifier_mixed.keras')\n",
    "        mlp_model.save(mlp_path)\n",
    "        print(f'Keras MLP saved -> {mlp_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error training Keras MLP: {e}')\n",
    "        traceback.print_exc()\n",
    "        results['Keras_MLP_Mixed'] = {k: float('nan') for k in ['Accuracy','Precision (w)','Recall (w)','F1-Score (w)','Train Time (s)']}\n",
    "else:\n",
    "    print('Skipping Keras MLP (TensorFlow not available).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 1D CNN Training (Mixed Precision)\n",
    "\n",
    "A 1D Convolutional Neural Network treating each pixel's spectrum as a 1D signal of length 5.\n",
    "Architecture: `Conv1D(32) → Pool → Conv1D(64) → Pool → Dense(128) → Dense(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = None\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    print('--- Training 1D CNN (Mixed Precision) ---')\n",
    "    cnn_start = time.time()\n",
    "    try:\n",
    "        X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "        X_test_cnn  = X_test_scaled.reshape(X_test_scaled.shape[0],  X_test_scaled.shape[1],  1)\n",
    "        cnn_model = build_cnn_classifier((X_train_cnn.shape[1], 1))\n",
    "        cnn_model.summary()\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss', patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True, verbose=1,\n",
    "        )\n",
    "        cnn_model.fit(\n",
    "            X_train_cnn, y_train,\n",
    "            epochs=CNN_EPOCHS, batch_size=BATCH_SIZE,\n",
    "            validation_split=VALIDATION_SPLIT,\n",
    "            callbacks=[early_stop], verbose=2,\n",
    "        )\n",
    "        cnn_duration = time.time() - cnn_start\n",
    "        y_pred_cnn = (cnn_model.predict(X_test_cnn).flatten() > 0.5).astype(int)\n",
    "        results['1D_CNN_Mixed'] = {\n",
    "            'Accuracy':       accuracy_score(y_test, y_pred_cnn),\n",
    "            'Precision (w)':  precision_score(y_test, y_pred_cnn, average='weighted', zero_division=0),\n",
    "            'Recall (w)':     recall_score(y_test, y_pred_cnn, average='weighted', zero_division=0),\n",
    "            'F1-Score (w)':   f1_score(y_test, y_pred_cnn, average='weighted', zero_division=0),\n",
    "            'Train Time (s)': cnn_duration,\n",
    "        }\n",
    "        print(classification_report(y_test, y_pred_cnn, target_names=le.classes_, digits=4))\n",
    "        plot_cm(y_test, y_pred_cnn, le.classes_, '1D_CNN_Mixed')\n",
    "        cnn_path = os.path.join(OUTPUT_DIR_MODELS, 'cnn_classifier_mixed.keras')\n",
    "        cnn_model.save(cnn_path)\n",
    "        print(f'1D CNN saved -> {cnn_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error training 1D CNN: {e}')\n",
    "        traceback.print_exc()\n",
    "        results['1D_CNN_Mixed'] = {k: float('nan') for k in ['Accuracy','Precision (w)','Recall (w)','F1-Score (w)','Train Time (s)']}\n",
    "else:\n",
    "    print('Skipping 1D CNN (TensorFlow not available).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TFLite Quantization\n",
    "\n",
    "Both Keras models are quantized to two compact TFLite formats for edge deployment:\n",
    "- **INT8** — smallest size, fastest CPU inference, uses a calibration dataset (1000 samples).\n",
    "- **FLOAT16** — half the size of FP32, compatible with GPU delegate on mobile devices.\n",
    "\n",
    "A temporary FP32 copy of each model is created (from the trained float16 weights) before quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_fp32_copy(keras_path):\n",
    "    \"\"\"Return a FP32 copy of a mixed-precision Keras model.\"\"\"\n",
    "    try:\n",
    "        mx = tf.keras.models.load_model(keras_path)\n",
    "        w  = mx.get_weights()\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {keras_path}: {e}'); return None\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    try:\n",
    "        fp32 = build_cnn_classifier(mx.input_shape[1:]) if 'cnn' in keras_path \\\n",
    "               else build_mlp_classifier(mx.input_shape[1:])\n",
    "        fp32.set_weights(w)\n",
    "    except Exception as e:\n",
    "        print(f'Error building FP32 model: {e}'); fp32 = None\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    return fp32\n",
    "\n",
    "\n",
    "def _quantize_int8(fp32_model, out_path, rep_gen):\n",
    "    \"\"\"Convert to INT8 TFLite.\"\"\"\n",
    "    c = tf.lite.TFLiteConverter.from_keras_model(fp32_model)\n",
    "    c.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    c.representative_dataset = rep_gen\n",
    "    c.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    c.inference_input_type = c.inference_output_type = tf.float32\n",
    "    with open(out_path, 'wb') as f: f.write(c.convert())\n",
    "    print(f'INT8  saved -> {out_path}')\n",
    "\n",
    "\n",
    "def _quantize_f16(fp32_model, out_path):\n",
    "    \"\"\"Convert to FLOAT16 TFLite.\"\"\"\n",
    "    c = tf.lite.TFLiteConverter.from_keras_model(fp32_model)\n",
    "    c.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    c.target_spec.supported_types = [tf.float16]\n",
    "    with open(out_path, 'wb') as f: f.write(c.convert())\n",
    "    print(f'F16   saved -> {out_path}')\n",
    "\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    N_REP = min(1000, len(X_train_scaled))\n",
    "\n",
    "    # ── MLP ──\n",
    "    mlp_keras = os.path.join(OUTPUT_DIR_MODELS, 'mlp_classifier_mixed.keras')\n",
    "    if os.path.exists(mlp_keras):\n",
    "        print('--- Quantizing MLP ---')\n",
    "        fp32_mlp = _get_fp32_copy(mlp_keras)\n",
    "        if fp32_mlp:\n",
    "            def _rep_mlp():\n",
    "                for i in range(N_REP):\n",
    "                    yield [X_train_scaled[i:i+1].astype(np.float32)]\n",
    "            _quantize_int8(fp32_mlp, os.path.join(OUTPUT_DIR_MODELS, 'mlp_classifier_int8.tflite'), _rep_mlp)\n",
    "            _quantize_f16(fp32_mlp,  os.path.join(OUTPUT_DIR_MODELS, 'mlp_classifier_f16.tflite'))\n",
    "\n",
    "    # ── CNN ──\n",
    "    cnn_keras = os.path.join(OUTPUT_DIR_MODELS, 'cnn_classifier_mixed.keras')\n",
    "    if os.path.exists(cnn_keras):\n",
    "        print('--- Quantizing CNN ---')\n",
    "        fp32_cnn = _get_fp32_copy(cnn_keras)\n",
    "        if fp32_cnn:\n",
    "            def _rep_cnn():\n",
    "                for i in range(N_REP):\n",
    "                    s = X_train_scaled[i:i+1]\n",
    "                    yield [s.reshape(s.shape[0], s.shape[1], 1).astype(np.float32)]\n",
    "            _quantize_int8(fp32_cnn, os.path.join(OUTPUT_DIR_MODELS, 'cnn_classifier_int8.tflite'), _rep_cnn)\n",
    "            _quantize_f16(fp32_cnn,  os.path.join(OUTPUT_DIR_MODELS, 'cnn_classifier_f16.tflite'))\n",
    "else:\n",
    "    print('Skipping TFLite quantization (TensorFlow not available).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. XGBoost GPU Training\n",
    "\n",
    "GPU-accelerated gradient boosting using `device='cuda'` and `tree_method='hist'`.\n",
    "A 5-fold stratified `GridSearchCV` searches over 48 hyperparameter combinations.\n",
    "`scale_pos_weight` is automatically calculated from the class distribution to handle the dataset imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = None\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    print('--- Training XGBoost GPU ---')\n",
    "\n",
    "    # Class-imbalance weight\n",
    "    count_neg = np.sum(y_train != sargassum_encoded_value)\n",
    "    count_pos = np.sum(y_train == sargassum_encoded_value)\n",
    "    scale_pos_weight = count_neg / count_pos if count_pos > 0 else 1.0\n",
    "    print(f'Class ratio (neg/pos) = {scale_pos_weight:.2f}')\n",
    "\n",
    "    xgb_template = xgb.XGBClassifier(\n",
    "        random_state=SEED,\n",
    "        device='cuda',\n",
    "        tree_method='hist',\n",
    "        eval_metric='logloss',\n",
    "    )\n",
    "    param_grid = {\n",
    "        'n_estimators':     [100, 150],\n",
    "        'learning_rate':    [0.05, 0.1],\n",
    "        'max_depth':        [3, 5, 7],\n",
    "        'subsample':        [0.7, 1.0],\n",
    "        'colsample_bytree': [0.7, 1.0],\n",
    "        'scale_pos_weight': [scale_pos_weight],\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    gs = GridSearchCV(xgb_template, param_grid, cv=cv, scoring='f1_weighted', n_jobs=1, verbose=1)\n",
    "\n",
    "    xgb_start = time.time()\n",
    "    try:\n",
    "        gs.fit(X_train_scaled, y_train)\n",
    "        xgb_best = gs.best_estimator_\n",
    "        xgb_duration = time.time() - xgb_start\n",
    "        print(f'Best params:  {gs.best_params_}')\n",
    "        print(f'Best CV F1 (weighted): {gs.best_score_:.4f}')\n",
    "\n",
    "        y_pred_xgb = xgb_best.predict(X_test_scaled)\n",
    "        results['XGBoost_GPU'] = {\n",
    "            'Accuracy':       accuracy_score(y_test, y_pred_xgb),\n",
    "            'Precision (w)':  precision_score(y_test, y_pred_xgb, average='weighted', zero_division=0),\n",
    "            'Recall (w)':     recall_score(y_test, y_pred_xgb, average='weighted', zero_division=0),\n",
    "            'F1-Score (w)':   f1_score(y_test, y_pred_xgb, average='weighted', zero_division=0),\n",
    "            'Train Time (s)': xgb_duration,\n",
    "        }\n",
    "        print(classification_report(y_test, y_pred_xgb, target_names=le.classes_, digits=4))\n",
    "        plot_cm(y_test, y_pred_xgb, le.classes_, 'XGBoost_GPU')\n",
    "\n",
    "        xgb_path = os.path.join(OUTPUT_DIR_MODELS, 'xgboost_gpu_classifier.joblib')\n",
    "        joblib.dump(xgb_best, xgb_path)\n",
    "        print(f'XGBoost saved -> {xgb_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error training XGBoost: {e}')\n",
    "        traceback.print_exc()\n",
    "        results['XGBoost_GPU'] = {k: float('nan') for k in ['Accuracy','Precision (w)','Recall (w)','F1-Score (w)','Train Time (s)']}\n",
    "else:\n",
    "    print('Skipping XGBoost (not available).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Summary\n",
    "\n",
    "Compile all test-set metrics into one table. Metrics are **weighted-averaged** across both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    cols = ['Accuracy', 'Precision (w)', 'Recall (w)', 'F1-Score (w)', 'Train Time (s)']\n",
    "    results_df = results_df[[c for c in cols if c in results_df.columns]]\n",
    "\n",
    "    print('--- Overall Test Set Performance ---')\n",
    "    display(results_df.sort_values('F1-Score (w)', ascending=False).style.format('{:.4f}'))\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR_TABLES, 'classification_performance_summary.csv')\n",
    "    results_df.to_csv(out_path, float_format='%.4f')\n",
    "    print(f'Performance summary saved to: {out_path}')\n",
    "else:\n",
    "    print('No results to display — all models were skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Training is complete. All models and artefacts have been saved to `output/models_classification/`:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `mlp_classifier_mixed.keras` | Full Keras MLP (float16 weights) |\n",
    "| `mlp_classifier_int8.tflite` | INT8-quantized TFLite MLP |\n",
    "| `mlp_classifier_f16.tflite` | FLOAT16-quantized TFLite MLP |\n",
    "| `cnn_classifier_mixed.keras` | Full Keras 1D CNN (float16 weights) |\n",
    "| `cnn_classifier_int8.tflite` | INT8-quantized TFLite CNN |\n",
    "| `cnn_classifier_f16.tflite` | FLOAT16-quantized TFLite CNN |\n",
    "| `xgboost_gpu_classifier.joblib` | XGBoost classifier |\n",
    "| `scaler_classification.joblib` | Feature StandardScaler |\n",
    "| `label_encoder_classification.joblib` | Label encoder |\n",
    "\n",
    "Proceed to **`Notebook_2_classify_sargassum.ipynb`** to apply these models to satellite imagery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}